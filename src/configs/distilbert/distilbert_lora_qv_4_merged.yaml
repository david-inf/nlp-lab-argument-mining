batch_size: 32
checkpoint: null
checkpoint_dir: src/ckps/distilbert
checkpoint_every: null
dataset: merged
early_stopping:
  min_delta: 0.002
  patience: 5
experiment_name: distilbert_lora_qv_4_merged
ft_setting:
  alpha: 8
  ftname: lora_qv_4
  lr_head: 0.01
  rank: 2
  target_modules:
  - q_lin
  - v_lin
  type: lora
learning_rate: 5.0e-05
log_every: 20
max_length: 128
model: distilbert
num_epochs: 30
num_workers: 4
seed: 42
warmup: 0.05
weight_decay: 0.01
