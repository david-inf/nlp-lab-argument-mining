batch_size: 32
checkpoint: src/ckps/distilbert/distilbert_lora_q_2_aae2
checkpoint_dir: src/ckps/distilbert
checkpoint_every: null
config_file: src/configs/distilbert/distilbert_lora_q_2_aae2.yaml
dataset: aae2
early_stopping:
  min_delta: 0.002
  patience: 5
experiment_name: distilbert_lora_q_2_aae2
ft_setting:
  alpha: 8
  ftname: lora_q_2
  rank: 2
  target_modules:
  - q_lin
  type: lora
learning_rate: 5.0e-05
log_every: 20
max_length: 128
model: distilbert
num_epochs: 30
num_workers: 4
seed: 42
visualize: false
warmup: 0.05
weight_decay: 0.01
