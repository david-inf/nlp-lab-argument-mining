batch_size: 32
checkpoint: null
checkpoint_dir: src/ckps/distilbert
checkpoint_every: null
dataset: merged
early_stopping:
  min_delta: 0.002
  patience: 5
experiment_name: distilbert_full_merged
ft_setting:
  ftname: full
  lr_head: 0.01
  type: full
learning_rate: 5.0e-05
log_every: 20
max_length: 128
model: distilbert
num_epochs: 30
num_workers: 4
seed: 42
warmup: 0.05
weight_decay: 0.01
